{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = open('src-dev.txt').read().split('\\n')\n",
    "X_test = open('src-test.txt').read().split('\\n')\n",
    "y_train = open('tgt-dev.txt').read().split('\\n')\n",
    "y_test = open('tgt-test.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8528\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "tokens = set([])\n",
    "for sentence in X_train+X_test:\n",
    "    for word in sentence.split(' '):\n",
    "        tokens.add(word)\n",
    "tokens = list(tokens)\n",
    "num_tokens = len(tokens)\n",
    "print(num_tokens)\n",
    "\n",
    "tags = set([])\n",
    "for sentence in y_train+y_test:\n",
    "    for tag in sentence.split(' '):\n",
    "        tags.add(tag)\n",
    "tags = list(tags)\n",
    "num_tags = len(tags)\n",
    "print(num_tags)    \n",
    "    \n",
    "def get_one_hot(word):\n",
    "    one_hot = torch.zeros(num_tokens)\n",
    "    one_hot[tokens.index(word)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_tag(tag):\n",
    "    return tags.index(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=8528, out_features=1000, bias=True)\n",
       "  (fc3): Linear(in_features=1000, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_tokens, 1000)\n",
    "        self.fc3 = nn.Linear(1000,num_tags)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = Variable(x.T)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)\n",
    "        \n",
    "net = Net()\n",
    "net = net.float()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# print(criterion(torch.tensor([1,0]),torch.tensor([0.9,0.1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4db70f9f5cd7>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8749, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0246, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7957, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0357, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1145, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8265, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9468, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9724, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9830, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8166, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8444, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0735, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6411, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0961, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9989, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8767, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9147, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0801, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0895, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1033, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0308, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9971, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9814, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9957, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0320, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9912, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1354, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9194, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0595, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0722, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7854, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0827, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1137, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0549, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0576, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0650, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9991, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0514, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0444, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1145, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9556, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8578, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7651, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8541, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7815, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9177, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9192, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8136, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9345, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9755, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0518, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7196, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8481, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7335, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8501, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0601, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1051, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8597, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9205, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0890, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8627, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1171, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1244, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0923, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1524, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8886, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0827, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0212, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8468, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1196, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0680, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0805, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8583, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9256, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0030, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8096, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0185, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8777, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0042, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8944, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8951, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9562, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0498, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1057, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9801, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9568, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9975, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9365, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9800, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9396, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0883, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0859, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0803, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9012, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0781, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8960, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9792, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9825, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0070, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0431, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7121, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9682, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9547, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0426, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9691, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8992, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9892, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9181, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0724, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1045, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9543, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8448, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9539, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0359, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1136, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0050, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0997, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8473, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1035, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9762, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8844, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9156, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0531, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9721, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8679, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0277, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0734, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0226, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0757, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0650, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0058, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9327, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1467, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8158, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9425, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1024, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8707, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0917, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9752, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1463, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0632, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0332, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9362, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9230, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0888, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0801, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0488, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7173, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1057, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0866, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1290, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9223, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0369, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0204, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8804, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8752, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8717, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0021, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1397, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9002, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9176, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8545, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9644, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0756, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0808, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7854, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1114, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1223, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9807, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9279, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0839, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0939, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8178, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0939, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0697, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9388, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7589, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1298, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0408, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8491, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1106, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8753, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1306, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9325, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9452, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8845, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8423, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0866, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0422, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9782, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9212, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0403, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0075, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0525, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9981, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1134, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0019, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8877, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9353, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0673, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1400, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0392, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9921, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9447, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9648, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0537, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7928, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0452, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9750, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8595, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0570, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1014, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0964, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9004, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0293, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1058, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0938, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0451, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0167, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0378, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8378, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1053, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9381, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9234, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1204, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9864, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7812, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9382, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9481, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0432, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9887, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7907, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0438, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0830, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9305, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0880, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9828, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0732, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1457, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9732, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9323, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0077, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8929, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8182, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9365, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0170, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9483, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9132, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8230, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0257, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0313, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8420, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9750, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8741, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9131, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8782, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9474, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9711, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8593, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9195, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8748, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1166, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0396, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1126, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9407, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0326, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0965, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8188, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0491, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0804, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1081, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0962, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9063, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0701, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8906, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1278, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0932, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9810, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9300, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0400, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9664, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1339, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0537, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9775, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0279, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0785, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1211, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0426, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0431, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9435, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0726, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0601, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0838, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0854, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0775, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8634, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9191, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8901, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1149, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8813, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0640, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0884, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9111, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0248, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8560, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0275, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0701, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0518, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9831, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1000, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9184, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0262, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9302, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0486, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7284, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0210, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0460, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9175, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9672, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8281, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1354, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0850, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9675, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9527, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9596, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1130, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8938, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8699, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9121, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9414, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0537, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0782, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9587, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0639, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1354, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8884, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0579, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0371, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0240, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0306, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0272, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0885, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9709, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8456, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8931, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1594, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0612, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0289, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1113, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8651, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0288, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9569, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9506, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0611, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7583, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0370, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8533, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7605, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0193, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8084, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7646, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1045, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0183, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0607, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0318, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1188, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1059, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9138, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0948, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0489, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8952, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9897, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1071, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8509, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8248, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0454, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 69 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-59e70fc5307c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_testing/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_testing/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    916\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_testing/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_testing/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 69 is out of bounds."
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        words = X_train[i].split(' ')\n",
    "        tags = y_train[i].split(' ')\n",
    "        a = [net(get_one_hot(word)) for word in words]\n",
    "        out = torch.stack(a)\n",
    "        out.to(device)\n",
    "        b = [get_tag(tag) for tag in tags]\n",
    "        res = torch.tensor(b)\n",
    "        res.to(device)\n",
    "        loss = criterion(out,res)\n",
    "        \n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
